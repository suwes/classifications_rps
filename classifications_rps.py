# -*- coding: utf-8 -*-
"""classifications_rps.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pEIG5e83qxiQc1TGxN9yGmFdXJkNPnxm

#Proyek Akhir: Klasifikasi Gambar

Klasifikasi gambar rock paper scissors dengan teknik Jaringan Saraf Tiruan (Convolutional Neural Network) dengan menggunakan TensorFlow. Proyek akhir tugas pada Dasar Manchine Learning Dicoding sebagai syarat kelulusan. Untuk file latihan saya sendiri, dapat di lihat pada halaman [ini](https://colab.research.google.com/drive/19W3Ql2zDHlE3py71rVl6EnjRpgnfTw4i?usp=sharing)


## Suwes Muhammad Hafiz
##### Kunjungi sosial media saya
[Github](https://github.com/suwes) | [Medium](https://suwes.medium.com) |
[LinkedIn](https://www.linkedin.com/in/suwes) | [Instagram](https://www.instagram.com/suweshafiz) | [Email](suweshafiz@gmail.com)

### Import Library
"""

# Commented out IPython magic to ensure Python compatibility.
# library pengelola file
import os
import glob
import shutil
import zipfile
import warnings

# library manchine learning dan tensorflow
import tensorflow as tf
import keras
import keras.preprocessing

from keras.preprocessing import image
from keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras import models
from tensorflow.keras import layers
from tensorflow.keras import optimizers
from tensorflow.keras import callbacks
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv2D, MaxPooling2D, Activation, Dense, Flatten
from tensorflow.keras.layers import Dropout
from tensorflow.keras.optimizers import Adam

# library ploting dan file upload
import numpy as np
from google.colab import files
from keras.preprocessing import image
import matplotlib.pyplot as plt
import matplotlib.image as mpimg
# %matplotlib inline

"""### Mempersiapkan Dataset"""

!wget --no-check-certificate \
  https://github.com/dicodingacademy/assets/releases/download/release/rockpaperscissors.zip

"""### Mengekstrak Dataset"""

# mengekstrak file dataset bertipe zip
local_zip = '/content/rockpaperscissors.zip'
local_unzip = zipfile.ZipFile(local_zip, 'r')
local_unzip.extractall('/content/dataset')
local_unzip.close()

"""### Mengelola Direktori Dataset"""

# mengatur direktori dataset masing-masing label
BASE_DIR =  '/content/dataset/rockpaperscissors/'
dir_paper = os.path.join(BASE_DIR, 'paper')
dir_rock = os.path.join(BASE_DIR, 'rock')
dir_scissors = os.path.join(BASE_DIR, 'scissors')

# menghapus direktori yang tidak diperlukan
shutil.rmtree(os.path.join(BASE_DIR, 'rps-cv-images'))
os.remove(os.path.join(BASE_DIR, 'README_rpc-cv-images.txt'))

"""### Mengecek Isi Dataset"""

# menghitung jumlah file dataset mpada setiap label
# membuat list key value direktori dataset
datadir = [
    {
        'dir_name':'Paper',
        'dir_path':dir_paper,
    },
    {
        'dir_name':'Rock',
        'dir_path':dir_rock,
    },
    {
        'dir_name':'Scissors',
        'dir_path':dir_scissors,
    }
]

# menghitung total file data
datacount = 0
for root_dir, cur_dir, files in os.walk(BASE_DIR):
  datacount += len(files)
print('TOTAL DATASET IMAGE : ', datacount)

# menghitung total file data berdasarkan label
for i in range (len(datadir)):
  print('Total Dataset',datadir[i]['dir_name'], 'Image: ', len(os.listdir(datadir[i]['dir_path'])))

# Horizontal bar data gambar untuk visualisasi
dataimage = []
quantity = []
for j in range (len(datadir)):
  dataimage.append(datadir[j]['dir_name'])
  quantity.append(len(os.listdir(datadir[j]['dir_path'])))

plt.barh(dataimage,quantity)
plt.title('Total dataset image')
plt.ylabel('dataimage')
plt.xlabel('quantity')
plt.show()

"""### Melihat Sampel Dataset"""

# menampilkan tiga data image dari setiap label
cell = 3
rows = 3
img_index = 0

fig = plt.gcf()
fig.set_size_inches(cell * 3, rows * 3)

img_index += 3

view_paper = [os.path.join(dir_paper, fname) for fname in os.listdir(dir_paper)[img_index-3:img_index]]
view_rock = [os.path.join(dir_rock, fname) for fname in os.listdir(dir_rock)[img_index-3:img_index]]
view_scissors = [os.path.join(dir_paper, fname) for fname in os.listdir(dir_paper)[img_index-3:img_index]]

for i, img_path in enumerate(view_paper+view_rock+view_scissors):
  sp = plt.subplot(cell , rows, i + 1)
  sp.axis('Off')
  img = mpimg.imread(img_path)
  plt.imshow(img)

plt.show()

"""### Augmentasi Gambar"""

# membuat train dan validasi dengan image data generator
# dengan ukuran validasi 40% dari total dataset

validation_size = 0.4

Train_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 40,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    zoom_range = 0.2,
    shear_range = 0.2,
    brightness_range = [0.1,1],
    horizontal_flip=True,
    vertical_flip= True,
    fill_mode = 'nearest',
    validation_split = validation_size
)

Validation_datagen = ImageDataGenerator(
    rescale = 1./255,
    rotation_range = 40,
    width_shift_range = 0.2,
    height_shift_range = 0.2,
    zoom_range = 0.2,
    shear_range = 0.2,
    brightness_range = [0.1,1],
    horizontal_flip=True,
    vertical_flip= True,
    fill_mode = 'nearest',
    validation_split = validation_size
)

"""### Train and Validation Split"""

# membagi data antara train dan validasi
# dengan syarat validasi sebesar 40%
# data training memiliki 1314 sampel, dan data validasi sebanyak 874 sampel

Train_generator = Train_datagen.flow_from_directory(
    BASE_DIR,
    target_size = (150,150),
    batch_size = 32,
    class_mode = 'categorical',
    color_mode = 'rgb',
    shuffle = True,
    subset = 'training'
)

Validation_generator = Validation_datagen.flow_from_directory(
    BASE_DIR,
    target_size = (150,150),
    batch_size = 32,
    class_mode = 'categorical',
    color_mode = 'rgb',
    shuffle = False,
    subset = 'validation'
)

"""### Modeling dengan Jaringan Syaraf Tiruan *(Convolutional Neural Network (CNN))* """

# membuat model sequential
model = Sequential([
    # membangun hidden layer dengan Conv2D dan MaxPooling
    # dan menggunakan ReLU dan Softmax Activation
    tf.keras.layers.Conv2D(16, (3,3), activation='relu', input_shape=(150,150,3)),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(32, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(64, (3,3), activation='relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(128,(3,3), activation= 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Conv2D(256,(3,3), activation= 'relu'),
    tf.keras.layers.MaxPooling2D(2,2),
    tf.keras.layers.Flatten(),

    # dropout
    tf.keras.layers.Dropout(0.02),

    tf.keras.layers.Dense(512, activation='relu'),
    tf.keras.layers.Dense(3, activation='softmax')
])

"""### Model Compile"""

# penggunaan optimizer Adam dengan LR 0.00146
Adam(learning_rate=0.00150, name='Adam')
model.compile(
    metrics = ['accuracy'],
    loss = 'categorical_crossentropy',
    optimizer = 'Adam'
)

model.summary()

"""### Menerapkan Learning Rate Schedule (LRS) dan Callback"""

# membuat fungsi learning rate
def scheduler(epoch, lr):
  if epoch < 5:
    return lr
  else:
    return lr * tf.math.exp(-0.1)

# membangun callback dengan menggunakan tensorflow board
lr_schedule = tf.keras.callbacks.LearningRateScheduler(scheduler, verbose=1)
tb_callback = tf.keras.callbacks.TensorBoard(
    log_dir = 'logs',
    histogram_freq = 0,
    update_freq = 'epoch',
    write_graph = True,
    write_images = False,
    embeddings_freq = 0,
    embeddings_metadata = None
)

"""### Training Model"""

# training model dengan learning rate shedule dan callback
batch_size = 32

history = model.fit(
          Train_generator,
          steps_per_epoch = 1314//batch_size,
          epochs = 10,
          validation_data = Validation_generator,
          validation_steps = 874//batch_size,
          verbose = 1,
          callbacks = [lr_schedule, tb_callback]
          )

"""### Evaluasi Model"""

# mengevaluasi model dengan plot accuracy dan loss
log_acc = history.history['accuracy']
log_val_acc = history.history['val_accuracy']
log_loss = history.history['loss']
log_val_loss = history.history['val_loss']

epochs = range(len(log_acc))



# ploting accuracy
plt.title('training and validation accuracy')
plt.plot(epochs, log_acc, 'c', label='Train accuracy')
plt.plot(epochs, log_val_acc, 'b', label='Validation accuracy')
plt.legend(loc=0)
plt.figure()
plt.show()

# ploting loss
plt.title('training and validation loss')
plt.plot(epochs, log_loss, 'c', label='Train loss')
plt.plot(epochs, log_val_loss, 'b', label='Validation loss')
plt.legend(loc=0)
plt.figure()
plt.show()

# grouped bar min dan max accuracy
labels = ['Train Accuracy', 'Validation Accuracy']
min_acc = [(min(log_acc)),(min(log_val_acc))]
max_acc = [(max(log_acc)),(max(log_val_acc))]

xi = np.arange(len(labels))
width = 0.35

fig, ax = plt.subplots()
min_bar = ax.bar(xi - width/2, min_acc, width, label='Min')
max_bar = ax.bar(xi + width/2, max_acc, width, label='Max')

ax.set_ylabel('Accuracy')
ax.set_xlabel('Train in left and Val in right')
ax.set_title('Min-Max Train and Validation Accuracy')
ax.set_xticks(xi,labels)
ax.legend()

fig.tight_layout()
plt.show()



# menampilkan min dan max accuracy
def view_summary(nm,la,mm):
  print(nm, ' : ', float(f'{(mm(la)):.05f}'))

def ln(opt):
  if opt == 1:
    print("_"*50)
  elif opt == 2:
    print('='*50)
  else:
    print('-'*50)

print('\n')
ln(1)
print('SUMMARY'.center(50))
ln(2)
# memanggil fungsi view_summary
view_summary('Min train accuracy',log_acc,min)
view_summary('Max train accuracy',log_acc,max)
view_summary('Min validation accuracy',log_val_acc,min)
view_summary('Max validation accuracy',log_val_acc,max)
ln(1)
view_summary('Min train loss',log_loss,min)
view_summary('Max train loss',log_loss,max)
view_summary('Min validation loss',log_val_loss,min)
view_summary('Max validation loss',log_val_loss,max)
ln(1)

"""### Memprediksi Model"""

# membuat uploader file
import numpy as np                                                                
from google.colab import files                                                    
from keras.preprocessing import image
up = files.upload()

for file_name in up.keys():

  path = file_name
  img = image.load_img(path, target_size=(150,150))
  imgplot = plt.imshow(img)
  x = image.img_to_array(img)
  x = np.expand_dims(x, axis=0)

  # menampung hasil prediksi dengan numpy vstack
  images = np.vstack([x])
  classes = model.predict(images, batch_size=32)

  # menyimpulkan hasil prediksi
  ln(1)
  print('Nama file :', file_name)
  if classes[0,0]!=0:
    print('Ini Gambar : Kertas')
  elif classes[0,1]!=0:
    print('Ini Gambar : Batu')
  else:
    print('Ini Gambar : Gunting')
  ln(2)